<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Eita-nakamura.GitHub.io : ">
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Eita Nakamura</title>
 
  <style>
      ul.pub {line-height: 20px;}
      ul.pub li {margin-bottom: 10px;}
    </style>

    <script>
     (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

     ga('create', 'UA-80849161-1', 'auto');
    ga('send', 'pageview');

    </script>

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title">Eita Nakamura　　　　 <a href="index-ja.html"><font size="5" color="wheat">日本語へ</font></a></h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <div>
        <img src="images/EitaNakamura_portrait.jpg" align=left height="160" alt="Picture"/>
        <div>
          Assistant Professor<br>
          Speech and Audio Processing Group <a href="http://sap.ist.i.kyoto-u.ac.jp/EN/"><img src="./images/externallink.png" height="16px" alt="External Site"></a><br>
          Graduate School of Informatics, Kyoto University <a href="http://www.i.kyoto-u.ac.jp/en/"><img src="./images/externallink.png" height="16px" alt="External Site"></a><br>
          <font color="#ff7878">enakamura[at]sap.ist.i.kyoto-u.ac[dot]jp</font><br>
          <font size=4><a href="eita-nakamura_publications.html"><strong>Publications and Talks</strong></a> [<a href="http://arxiv.org/a/nakamura_e_1.html" target="_blank">arXiv</a>]</font><br>
          <a href="eita-nakamura_cv.html"><strong>Curriculum Vitae</strong></a><br>
        </div><br>
        </div><br>

        <h3>Research Interests</h3>
        <ul>
          <li>Models of Creative Intelligence
          <li>Statistical Learning
          <li>Evolutionary Dynamics
          <li>Music Information Processing
        </ul>

        <h3>Research Topics</h3>

        <div style="position:relative; width:800px; height:140px; margin: 20px 20px 20px 20px; padding: 0px 0px 0px 0px;">
          <img src="./images/MusicEvolution.jpg" height="130px" alt="" style="top:5px; box-shadow: 0 0 0px #ebebeb; margin: 0px 0px 0px 0px; border:1px solid rgb(200,200,200);">
          <div style="position:absolute; top:10px; left:330px; width:470px; height:120px; font-weight:300; text-align:justify;">
            <div style="font-size:16pt; font-weight:600; margin: 0px 0px 5px 0px;">Evolution of music</div>
            <strong>Music styles change over time. How and why do they change? Can we predict future music styles?</strong> We are studying the evolution of music based on computational analyses of music data and theoretical models of music creation process and social selection.
          </div>
        </div>

        <div style="position:relative; width:800px; height:180px; margin: 20px 20px 20px 20px; padding: 0px 0px 0px 0px;">
          <iframe width="300" height="180" src="https://www.youtube.com/embed/82K1GPnaSfM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <div style="position:absolute; top:10px; left:330px; width:450px; height:120px; font-weight:300; text-align:justify;">
            <div style="font-size:16pt; font-weight:600; margin: 0px 0px 5px 0px;">Automatic music generation</div>
            <strong>Can we compose music by computers?</strong> We are studying music creation from the viewpoints of statistical learning and optimization. We are interested in how individual music styles are developed and transmitted, and in the roles of <strong>active and dynamic statistical learning</strong> in musical creativity.<br>
            <strong><a href="https://melodyarrangement.github.io/demo-en.html" target="_blank">Melody style conversion</a></strong>　　
            <strong><a href="https://pianoarrangement.github.io/demo.html" target="_blank">Piano arrangement</a></strong>
          </div>
        </div>

        <div style="position:relative; width:800px; height:162px; margin: 20px 20px 20px 20px; padding: 0px 0px 0px 0px;">
          <img src="./images/MusicTranscription.png" height="160px" alt="" style="top:0px; left:10px; box-shadow: 0 0 0px #ebebeb; margin: 0px 0px 0px 0px; border:1px solid rgb(200,200,200);">
          <div style="position:absolute; top:10px; left:330px; width:450px; height:120px; font-weight:300; text-align:justify;">
            <div style="font-size:16pt; font-weight:600; margin: 0px 0px 5px 0px;">Music transcription</div>
            Trained musicians can listen to music audio and represent it as a musical score. We are studying computational methods to emulate this ability. Automatic music transcription is a <strong>key experimental method</strong> for analyzing music audio in the symbolic domain.<br>
            <a href="http://anonymous4721029.github.io/demo.html" target="_blank"><img src="./images/demo.png" height="18px" alt="Demo"></a> <a href="https://anonymous574868.github.io/demo.html" target="_blank"><img src="./images/demo.png" height="18px" alt=""></a>
          </div>
        </div>

        <div style="position:relative; width:800px; height:180px; margin: 20px 20px 20px 20px; padding: 0px 0px 0px 0px;">
          <iframe width="300" height="180" src="https://www.youtube.com/embed/KgnR2BzrafU?start=167" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <div style="position:absolute; top:10px; left:330px; width:450px; height:120px; font-weight:300; text-align:justify;">
            <div style="font-size:16pt; font-weight:600; margin: 0px 0px 5px 0px;">Automatic music accompaniment</div>
            <strong>Can computers listen to music and interact with humans?</strong> We are studying a method for understanding human performances and playing accompaniments with humans. Various aspects of human performances including tempo changes, errors, repeats and skips, ornaments are studied.<br>
            <strong><a href="http://hil.t.u-tokyo.ac.jp/software/Eurydice/index-e.html">Eurydice system</a></strong>
          </div>
        </div>

        <div style="position:relative; width:800px; height:160px; margin: 20px 20px 20px 20px; padding: 0px 0px 0px 0px;">
          <img src="./images/PianoFingering.png" width="300px" alt="" style="top:5px; left:0px; box-shadow: 0 0 0px #ebebeb; margin: 0px 0px 0px 0px; border:1px solid rgb(200,200,200);">
          <div style="position:absolute; top:10px; left:330px; width:450px; height:120px; font-weight:300; text-align:justify;">
            <div style="font-size:16pt; font-weight:600; margin: 0px 0px 5px 0px;">Piano fingering</div>
           Fingering is a basic skill for pianists, but the principle of finding appropriate fingerings is not fully understood scientifically. We are constructing computational models to <strong>formulate and search for optimal fingerings</strong> and to <strong>measure performance difficulty</strong>.<br>
            <a href="http://beam.kisarazu.ac.jp/research/PianoFingeringDataset/" target="_blank"><img src="./images/PIGDatasetLogo-2.png" height="20px" alt="http://beam.kisarazu.ac.jp/research/PianoFingeringDataset/"></a>
          </div>
        </div>

        <div style="position:relative; width:800px; height:140px; margin: 20px 20px 20px 20px; padding: 0px 0px 0px 0px;">
          <img src="./images/SymbolicMusicAlignment.jpg" height="140px" alt="" style="top:0px; left:25px; box-shadow: 0 0 0px #ebebeb; margin: 0px 0px 0px 0px; border:1px solid rgb(200,200,200);">
          <div style="position:absolute; top:10px; left:330px; width:450px; height:120px; font-weight:300; text-align:justify;">
            <div style="font-size:16pt; font-weight:600; margin: 0px 0px 5px 0px;">Symbolic music alignment</div>
            Study for <strong>comparing two symbolic representations</strong> of a musical piece (for example a MIDI recording and a corresponding musical score) at the note level.<br>
            <strong><a href="https://midialignment.github.io/demo.html" target="_blank">Software and demo</a></strong>
          </div>
        </div>

        <div style="font-size:10pt;">　</div>



        <h3>Selected Publications and Talks</h3>
        <h5>(See the complete list <a href="eita-nakamura_publications.html">here</a>)</h5>
        <ul class="pub">


          <h5><font color="#990000">2019</font></h5>

<li>Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><a href="https://arxiv.org/pdf/1908.06969.pdf" target="_blank"><font color="#222222">Music Transcription Based on Bayesian Piece-Specific Score Models Capturing Repetitions</font></a></strong> <a href="https://arxiv.org/pdf/1908.06969.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a><br>
<em>Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing</em> <a href="https://arxiv.org/abs/1908.06969" target="_blank"><font color="#808080">[arXiv:1908.06969]</font></a>

<li>Ryo Nishikimi, Eita Nakamura, Masataka Goto, Katsutoshi Itoyama, Kazuyoshi Yoshii<br>
<strong><font color="#222222">Bayesian Singing Transcription Based on a Hierarchical Generative Model of Keys, Musical Notes, and F0 Trajectories</font></strong><br>
<em>Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing</em>

<li>Eita Nakamura, Yasuyuki Saito, Kazuyoshi Yoshii<br>
<strong><a href="https://arxiv.org/pdf/1904.10237.pdf" target="_blank"><font color="#222222">Statistical Learning and Estimation of Piano Fingering</font></a></strong> <a href="https://arxiv.org/pdf/1904.10237.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a><br>
<em>Submitted to Information Sciences</em> <a href="https://arxiv.org/abs/1904.10237" target="_blank"><font color="#808080">[arXiv:1904.10237]</font></a>

<li>Hiroaki Tsushima, Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><font color="#222222">Bayesian Melody Harmonization Based on a Tree-Structured Generative Model of Chord Sequences and Melodies</font></strong><br>
<em>Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing</em>

<li>Eita Nakamura, Kunihiko Kaneko<br>
<strong><a href="https://arxiv.org/pdf/1809.05832.pdf" target="_blank"><font color="#222222">Statistical Evolutionary Laws in Music Styles</font></a></strong> <a href="https://arxiv.org/pdf/1809.05832.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a><br>
<a href="https://arxiv.org/abs/1809.05832" target="_blank"><font color="#808080">[arXiv:1809.05832]</font></a>

<li>Tristan Carsault, Andrew McLeod, Philippe Esling, Jérôme Nika, Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><font color="#222222">Multi-Step Chord Sequence Prediction Based on Aggregated Multi-Scale Encoder-Decoder Networks</font></strong><br>
<em>Proc. 29th IEEE International Workshop on Machine Learning for Signal Processing (MLSP)</em>, to be presented, October 2019.

<li>Ryo Nishikimi, Eita Nakamura, Masataka Goto, Kazuyoshi Yoshii<br>
<strong><font color="#222222">End-to-End Melody Note Transcription Based on a Beat-Synchronous Attention Mechanism</font></strong><br>
<em>Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>, to be presented, October 2019.

<li>Go Shibata, Ryo Nishikimi, Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><font color="#222222">Statistical Music Structure Analysis Based on a Homogeneity- and Repetitiveness-Aware Hierarchical Hidden Semi-Markov Model</font></strong><br>
<em>Proc. 20th International Society for Music Information Retrieval Conference (ISMIR)</em>, to be presented, November 2019.

<li>Yui Uehara, Eita Nakamura, Satoshi Tojo<br>
<strong><font color="#222222">Chord Function Identification with Modulation Detection Based on HMM</font></strong><br>
<em>Proc. 14th International Symposium on Computer Music Multidisciplinary Research (CMMR)</em>, to be presented, October 2019.

<li>Eita Nakamura, Kentaro Shibata, Ryo Nishikimi, Kazuyoshi Yoshii<br>
<strong><a href="./articles/Nakamura_etal_StatisticalMelodyArrangement_ICASSP2019.pdf" target="_blank"><font color="#222222">Unsupervised Melody Style Conversion</font></a></strong> <a href="./articles/Nakamura_etal_StatisticalMelodyArrangement_ICASSP2019.pdf" target="_blank"><img src="./images/pdf.png" height="18px"></a> <a href="https://melodyarrangement.github.io/demo.html" target="_blank"><img src="./images/demo.png" height="18px" alt="Demo"></a> <a href="https://www.youtube.com/watch?v=82K1GPnaSfM" target="_blank"><img src="./images/video.png" height="18px" alt="Video"></a><br>
<em>Proc. 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 196-200, May 2019.

<li>Kentaro Shibata, Ryo Nishikimi, Satoru Fukayama, Masataka Goto, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii<br>
<strong><a href="./articles/Shibata_etal_GuitarTranscription_ICASSP2019.pdf" target="_blank"><font color="#222222">Joint Transcription of Lead, Bass, and Rhythm Guitars Based on a Factorial Hidden Semi-Markov Model</font></a></strong> <a href="./articles/Shibata_etal_GuitarTranscription_ICASSP2019.pdf" target="_blank"><img src="./images/pdf.png" height="18px"></a><br>
<em>Proc. 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 236-240, May 2019.

<li>Andrew McLeod, Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><a href="https://ieeexplore.ieee.org/document/8683808" target="_blank"><font color="#222222">Improved Metrical Alignment of MIDI Performance Based on a Repetition-Aware Online-Adapted Grammar</font></a></strong><br>
<em>Proc. 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 186-190, May 2019.

<li>Ryo Nishikimi, Eita Nakamura, Satoru Fukayama, Masataka Goto, Kazuyoshi Yoshii<br>
<strong><a href="./articles/Nishikimi_etal_SingingTranscriptionUsingAttention_ICASSP2019.pdf" target="_blank"><font color="#222222">Automatic Singing Transcription Based on Encoder-Decoder Recurrent Neural Networks with a Weakly-Supervised Attention Mechanism</font></a></strong> <a href="./articles/Nishikimi_etal_SingingTranscriptionUsingAttention_ICASSP2019.pdf" target="_blank"><img src="./images/pdf.png" height="18px"></a><br>
<em>Proc. 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 161-165, May 2019.

<li>Shun Ueda, Kentaro Shibata, Yusuke Wada, Ryo Nishikimi, Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><a href="./articles/Ueda_etal_DrumTranscription_ICASSP2019.pdf" target="_blank"><font color="#222222">Bayesian Drum Transcription Based on Nonnegative Matrix Factor Decomposition with a Deep Score Prior</font></a></strong> <a href="./articles/Ueda_etal_DrumTranscription_ICASSP2019.pdf" target="_blank"><img src="./images/pdf.png" height="18px"></a><br>
<em>Proc. 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 456-460, May 2019.

          <h5><font color="#990000">2018 and before</font></h5>

<li>Eita Nakamura, Kazuyoshi Yoshii<br>
<strong><a href="https://arxiv.org/pdf/1808.05006.pdf" target="_blank"><font color="#222222">Statistical Piano Reduction Controlling Performance Difficulty</font></a></strong> <a href="https://arxiv.org/pdf/1808.05006.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="https://doi.org/10.1017/ATSIP.2018.18" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a> <a href="https://pianoarrangement.github.io/demo.html" target="_blank"><img src="./images/demo.png" height="18px" alt="Demo"></a><br>
<em>APSIPA Transactions on Signal and Information Processing</em>, Vol. 7, No. e13, pp. 1–12, 2018. <a href="https://arxiv.org/abs/1808.05006" target="_blank"><font color="#808080">[arXiv:1808.05006]</font> <a href="./citations/2018PianoReduction.txt" target="_blank"><img src="./images/citation.png" height="18px" alt=""></a>

<li>Eita Nakamura, Emmanouil Benetos, Kazuyoshi Yoshii, Simon Dixon<br>
<strong><a href="./articles/AudioAndMIDITranscription_ICASSP2018.pdf" target="_blank"><font color="#222222">Towards Complete Polyphonic Music Transcription: Integrating Multi-Pitch Detection and Rhythm Quantization</font></a></strong> <a href="./articles/AudioAndMIDITranscription_ICASSP2018.pdf" target="_blank"><img src="./images/pdf.png" height="18px"></a><br>
<em>Proc. 43rd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 101-105, April, 2018.

<li>Hiroaki Tsushima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii<br>
<strong><a href="https://arxiv.org/pdf/1708.02255.pdf" target="_blank"><font color="#222222">Generative Statistical Models with Self-Emergent Grammar of Chord Sequences</font></a></strong> <a href="https://arxiv.org/pdf/1708.02255.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="https://www.tandfonline.com/doi/abs/10.1080/09298215.2018.1447584?journalCode=nnmr20" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a><br>
<em>Journal of New Music Research</em>, Vol. 47, No. 3, pp. 226–248, 2018. <a href="https://arxiv.org/abs/1708.02255" target="_blank"><font color="#808080">[arXiv:1708.02255]</font> <a href="./citations/2018ChordModel.txt" target="_blank"><img src="./images/citation.png" height="18px" alt=""></a>

<li>Eita Nakamura, Kazuyoshi Yoshii, Haruhiro Katayose<br>
<strong><a href="./articles/EN_etal_ErrorDetectionAndRealignment_ISMIR2017.pdf" target="_blank"><font color="#222222">Performance Error Detection and Post-Processing for Fast and Accurate Symbolic Music Alignment</font></a></strong> <a href="./articles/EN_etal_ErrorDetectionAndRealignment_ISMIR2017.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="https://midialignment.github.io/demo.html" target="_blank"><img src="./images/demo.png" height="18px" alt=""></a><br>
<em>Proc. 18th International Society for Music Information Retrieval Conference (ISMIR)</em>, pp. 347-353, 2017.

<li>Eita Nakamura, Kazuyoshi Yoshii, Shigeki Sagayama<br>
<strong><a href="https://arxiv.org/pdf/1701.08343v1.pdf" target="_blank"><font color="#222222">Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices</font></a></strong> <a href="https://arxiv.org/pdf/1701.08343v1.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="https://doi.org/10.1109/TASLP.2017.2662479" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a> <a href="http://anonymous4721029.github.io/demo.html" target="_blank"><img src="./images/demo.png" height="18px" alt=""></a><br>
<em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>, Vol. 25, No. 4, pp. 794-806, 2017. <a href="https://arxiv.org/abs/1701.08343" target="_blank"><font color="#808080">[arXiv:1701.08343]</font></a> <a href="./citations/2017PolyphonicRhythmTranscription.txt" target="_blank"><img src="./images/citation.png" height="18px" alt=""></a>

<li>Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii<br>
<strong><a href="./articles/Nakamura_etal_RhythmTranscriptionBasedOnHierarchicalBayesianModellingOfRepetitionAndModification_EUSIPCO2016.pdf" target="_blank"><font color="#222222">Rhythm Transcription of MIDI Performances Based on Hierarchical Bayesian Modelling of Repetition and Modification of Musical Note Patterns</font></a></strong> <a href="./articles/Nakamura_etal_RhythmTranscriptionBasedOnHierarchicalBayesianModellingOfRepetitionAndModification_EUSIPCO2016.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a><br>
<em>Proc. 24th European Signal Processing Conference (EUSIPCO)</em>, pp. 1946-1950, 2016.

<li>Eita Nakamura, Nobutaka Ono, Shigeki Sagayama, Kenji Watanabe<br>
<strong><a href="https://arxiv.org/pdf/1404.2314v2.pdf" target="_blank"><font color="#222222">A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments</font></a></strong> <a href="https://arxiv.org/pdf/1404.2314v2.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="http://www.tandfonline.com/doi/full/10.1080/09298215.2015.1078819" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a><br>
<em>Journal of New Music Research</em>, Vol. 44, No. 4, pp. 287-304, 2015. <a href="https://arxiv.org/abs/1404.2314" target="_blank"><font color="#808080">[arXiv:1404.2314]</font></a> <a href="./citations/2014OrnamentModel.txt" target="_blank"><img src="./images/citation.png" height="18px" alt=""></a>

<li>Eita Nakamura, Shinji Takaki<br>
<strong><a href="./articles/Nakamura-Takaki_PolyphonicMusicStyleAndPCIntervals_MCM2015.pdf" target="_blank"><font color="#222222">Characteristics of Polyphonic Music Style and Markov Model of Pitch-Class Intervals</font></a></strong> <a href="./articles/Nakamura-Takaki_PolyphonicMusicStyleAndPCIntervals_MCM2015.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="http://link.springer.com/chapter/10.1007/978-3-319-20603-5_10" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a><br>
<em>Proc. 5th Mathematics and Computation in Music (MCM)</em>, pp. 109-114, 2015.

<li>Eita Nakamura, Nobutaka Ono, Shigeki Sagayama<br>
<strong><a href="./articles/Nakamura_etal_MergedOutputHMMForPianoFingering_ISMIR2014.pdf" target="_blank"><font color="#222222">Merged-Output HMM for Piano Fingering of Both Hands</font></a></strong> <a href="./articles/Nakamura_etal_MergedOutputHMMForPianoFingering_ISMIR2014.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="http://www.terasoft.com.tw/conf/ismir2014/proceedings/T096_251_Paper.pdf" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a><br>
<em>Proc. 15th International Society for Music Information Retrieval Conference (ISMIR)</em>, pp. 531-536, 2014.

<li>Koichi Hamaguchi, Eita Nakamura, Satoshi Shirai, Tsutomu T. Yanagida<br>
<strong><a href="https://arxiv.org/pdf/0811.0737v2.pdf" target="_blank"><font color="#222222">Decaying Dark Matter Baryons in a Composite Messenger Model</font></a></strong> <a href="https://arxiv.org/pdf/0811.0737v2.pdf" target="_blank"><img src="./images/pdf.png" height="18px" alt=""></a> <a href="http://www.sciencedirect.com/science/article/pii/S0370269309003013" target="_blank"><img src="./images/externallink.png" height="18px" alt=""></a><br>
<em>Physics Letters B</em>, Vol. 674, pp. 299-302, 2009. <a href="https://arxiv.org/abs/0811.0737" target="_blank"><font color="#808080">[arXiv:0811.0737]</font></a>

        </ul>



        <h3>Contact</h3>
          Eita Nakamura<br>
          Research Building No. 7 Room 417, Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan<br>
          <strong>e-mail:</strong> enakamura[at]sap.ist.i.kyoto-u.ac[dot]jp<br>
          <strong>phone:</strong> +81-075-753-4952<br>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Maintained by Eita Nakamura　　　　(Last updated: Sep 2019)　　<img src="https://www.f-counter.net/j/31/1553852034/" alt="access counter"></p>
      </footer>
    </div>



  </body>
</html>
